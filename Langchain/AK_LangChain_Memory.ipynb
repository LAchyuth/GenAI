{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **LangChain Memory**\n",
        "\n",
        "Most LLM applications have a conversational interface. An essential component of a conversation is being able to refer to information introduced earlier in the conversation. At bare minimum, a conversational system should be able to access some window of past messages directly. A more complex system will need to have a world model that it is constantly updating, which allows it to do things like maintain information about entities and their relationships.\n",
        "\n",
        "We call this ability to store information about past interactions \"memory\". LangChain provides a lot of utilities for adding memory to a system. These utilities can be used by themselves or incorporated seamlessly into a chain.\n",
        "\n",
        "## **Building memory into a system**\n",
        "The two core design decisions in any memory system are:\n",
        "- How state is stored\n",
        "- How state is queried\n",
        "\n",
        "## **What's covered?**\n",
        "- ConversationBufferMemory\n",
        "- End-to-end Example: Conversational AI Bot\n",
        "- Saving and Loading a Chat History\n",
        "- ConversationBufferWindowMemory"
      ],
      "metadata": {
        "id": "g8funJmXJm_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ConversationBufferMemory**\n",
        "\n",
        "Let's take a look at how to use ConversationBufferMemory in chains. ConversationBufferMemory is an extremely simple form of memory that just keeps a list of chat messages in a buffer and passes those into the prompt template.\n",
        "\n",
        "This memory type can be connected to a conversation. It allows for storing messages and then extracts the messages in a variable."
      ],
      "metadata": {
        "id": "Kr---3koJsEy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **End-to-end Example: Conversational AI Bot**\n",
        "\n",
        "<img src=\"images/memory.png\">\n",
        "\n",
        "### **Steps:**\n",
        "1. Import Chat Model and Configure the API Key\n",
        "2. Create Chat Template\n",
        "3. Initialize the Memory\n",
        "4. Create a Output Parser\n",
        "5. Build a Chain\n",
        "6. Invoke the chain with human_input and chat_history\n",
        "7. Saving to memory\n",
        "8. Run Step 6 and 7 in a loop"
      ],
      "metadata": {
        "id": "bKUiHEewJsHI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1 : Import Chat Model and Configure the API Key"
      ],
      "metadata": {
        "id": "jGr4mk7OJsJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pqM4umKMJ5A",
        "outputId": "e7157093-8d90-4f02-b3c0-d2b4d8eb0d42"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.3.4-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.4.0,>=0.3.12 (from langchain)\n",
            "  Downloading langchain_core-0.3.12-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.136-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.15.2)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.12->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (4.12.2)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m993.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
            "Downloading langchain-0.3.4-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.12-py3-none-any.whl (407 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.7/407.7 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.136-py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.7/296.7 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading orjson-3.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: orjson, jsonpointer, h11, requests-toolbelt, jsonpatch, httpcore, httpx, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.4 langchain-core-0.3.12 langchain-text-splitters-0.3.0 langsmith-0.1.136 orjson-3.10.9 requests-toolbelt-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgghVbjwMR77",
        "outputId": "fdc2250a-9c34-46df-b3b4-7628be4c446f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.2.3-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.12 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.3.12)\n",
            "Collecting openai<2.0.0,>=1.52.0 (from langchain-openai)\n",
            "  Downloading openai-1.52.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain-openai) (0.1.136)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain-openai) (24.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain-openai) (2.9.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain-openai) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain-openai) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.52.0->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.52.0->langchain-openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.52.0->langchain-openai) (0.27.2)\n",
            "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.52.0->langchain-openai)\n",
            "  Downloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.52.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.52.0->langchain-openai) (4.66.5)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.52.0->langchain-openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.52.0->langchain-openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.52.0->langchain-openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.52.0->langchain-openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.52.0->langchain-openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.12->langchain-openai) (3.10.9)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.12->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.12->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.12->langchain-openai) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.2.3)\n",
            "Downloading langchain_openai-0.2.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.52.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.9/386.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, tiktoken, openai, langchain-openai\n",
            "Successfully installed jiter-0.6.1 langchain-openai-0.2.3 openai-1.52.0 tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1 - Import Chat Model and Configure the API Key"
      ],
      "metadata": {
        "id": "pyA-KG56TZ1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "from google.colab import userdata  # api_key has uploaded in google colab\n",
        "API_Key =userdata.get('OpenAIAPIKey')\n",
        "chat_model = ChatOpenAI(openai_api_key =API_Key)"
      ],
      "metadata": {
        "id": "1QyKVf9nJsME"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2  -Create Chat Template"
      ],
      "metadata": {
        "id": "xYX4XohBTU-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder,SystemMessagePromptTemplate,HumanMessagePromptTemplate\n",
        "from langchain_core.messages import SystemMessage"
      ],
      "metadata": {
        "id": "QWM8iGyqJsOL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ************* Example Starts****************"
      ],
      "metadata": {
        "id": "k-xAQnxgS-hT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "memory = ConversationBufferMemory()\n",
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ13xXiYJsQj",
        "outputId": "a4374b75-0cb8-4a82-a813-419adea71e47"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-ac7c6ea998a8>:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.buffer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Jk-TdSTOJsS_",
        "outputId": "0d877c4d-7bd9-4da4-ce30-b874c62ccf71"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory(memory_key = 'chat_history')\n",
        "memory.chat_memory.add_user_message(\"Hi\")\n",
        "memory.chat_memory.add_ai_message(\"Hi, What's up? How can i help you\")\n",
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEf8ZZy4JsVa",
        "outputId": "e9cd3a0b-85d5-4422-d5b3-6de8ed0a9266"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'chat_history': \"Human: Hi\\nAI: Hi, What's up? How can i help you\"}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.buffer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JHYo4fGTJsXv",
        "outputId": "995ea83a-9939-4006-ee79-228e0ee96208"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Human: Hi\\nAI: Hi, What's up? How can i help you\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory(memory_key = 'chat_history',return_messages = True)\n",
        "memory.chat_memory.add_user_message(\"Hi\")\n",
        "memory.chat_memory.add_ai_message(\"Hi, What's up? How can i help you\")\n",
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7oXJ88uJsaP",
        "outputId": "f4859da5-7e21-4337-9207-cf0e1ef8b69f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'chat_history': [HumanMessage(content='Hi', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hi, What's up? How can i help you\", additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***************** Example Ends *******************"
      ],
      "metadata": {
        "id": "Yn5mblXkQuqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
        "    SystemMessage(content = \"you are a chatbot having a conversation with a human\"),\n",
        "    # Creating a chat_history placeholder\n",
        "    MessagesPlaceholder(variable_name = 'chat_history'),\n",
        "    #Human Prompt\n",
        "    HumanMessagePromptTemplate.from_template(\"{human_input}\"),])\n"
      ],
      "metadata": {
        "id": "Ok8h-titRYRR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L-npIYkpRYTb"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3- Initialize the Memory"
      ],
      "metadata": {
        "id": "Oogq8zmnRYVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "memory = ConversationBufferMemory(memory_key = 'chat_history',return_messages = True)\n"
      ],
      "metadata": {
        "id": "s8zTU1wTToiW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4 - Create a Output Parser"
      ],
      "metadata": {
        "id": "hJOopBGtTvBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "output_parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "h1IRAA41RYaT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5 -Build a Chain(New Method)"
      ],
      "metadata": {
        "id": "UYWf00nuRYcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "\n",
        "# Define a function to load the message from memory\n",
        "def get_messages_from_memory(human_input):\n",
        "    return memory.load_memory_variables(human_input)['chat_history']\n",
        "\n",
        "# Define a chain\n",
        "chain = RunnablePassthrough.assign(chat_history = RunnableLambda(get_messages_from_memory)) |  chat_prompt_template | chat_model | output_parser"
      ],
      "metadata": {
        "id": "qPrVvB2oRYex"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6 -Invoke the chain with human_input and chat_history"
      ],
      "metadata": {
        "id": "gxksxgr9RYg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query= {\"human_input\":\"Hi, How are you?\"}\n",
        "response = chain.invoke(query)\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MGn0jkp5RYjF",
        "outputId": "eb5aae79-47e3-46dc-99df-3777a8a10b11"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to chat with you. How are you doing today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7 -Saving to Memory"
      ],
      "metadata": {
        "id": "nJZkmIysRYlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context(query, {'output':response})\n",
        "memory.buffer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzBMAhMvRYnX",
        "outputId": "d1d1208d-a10b-4433-e25a-54634dfa492f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, How are you?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to chat with you. How are you doing today?\", additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8 -Run Step 6 and 7 in a loop"
      ],
      "metadata": {
        "id": "WfKVa_hyRYqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  query = {\"human_input\":input(\"Enter a query:  \")}\n",
        "  print(f\"User:{query['human_input']}\")\n",
        "  if query['human_input'] in ['bye','quit','exit']:\n",
        "    break\n",
        "  response = chain.invoke(query)\n",
        "  print(f\"AI:{response}\")\n",
        "  memory.save_context(query,{'output':response})"
      ],
      "metadata": {
        "id": "SmhnI1MERYr-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60c64fbd-eb5a-4065-ecbd-7bb3dccedac0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a query:  hi,how are you\n",
            "User:hi,how are you\n",
            "AI:I'm just a bot, so I don't have feelings, but I'm here to help you with any questions or just have a chat. How can I assist you today?\n",
            "Enter a query:  my name is achyuth, i want you to help with data wraningling  techniques\n",
            "User:my name is achyuth, i want you to help with data wraningling  techniques\n",
            "AI:Of course, Achyuth! I'd be happy to help you with data wrangling techniques. Data wrangling involves cleaning, transforming, and organizing raw data into a usable format for analysis. \n",
            "\n",
            "Some common data wrangling techniques include:\n",
            "- Handling missing values\n",
            "- Removing duplicates\n",
            "- Standardizing data formats\n",
            "- Merging datasets\n",
            "- Filtering and sorting data\n",
            "- Creating new variables\n",
            "- Handling outliers\n",
            "\n",
            "Is there a specific aspect of data wrangling you'd like to learn more about or need help with? Feel free to ask any questions you have!\n",
            "Enter a query:  explain oops concept in 5lines and why we need to learn that\n",
            "User:explain oops concept in 5lines and why we need to learn that\n",
            "AI:Object-Oriented Programming (OOP) is a programming paradigm based on the concept of \"objects,\" which can contain data and code to manipulate that data. OOP promotes modularity, reusability, and scalability in software development by organizing code into classes and objects. \n",
            "\n",
            "Learning OOP is important because it helps in creating more organized and efficient code, improves code reusability, enhances code readability, and makes it easier to maintain and debug large software projects. OOP also allows for better modeling of real-world entities and relationships in software applications.\n",
            "Enter a query:  explain the word2vec and seq2seq model\n",
            "User:explain the word2vec and seq2seq model\n",
            "AI:Word2Vec is a popular word embedding technique in natural language processing that represents words as vectors in a high-dimensional space. It captures semantic relationships between words by placing similar words closer together in the vector space. Word2Vec models are trained on large text corpora using neural networks, such as the Skip-gram or Continuous Bag of Words (CBOW) models.\n",
            "\n",
            "Seq2Seq (Sequence-to-Sequence) is a neural network architecture used for tasks involving sequences, such as machine translation, text summarization, and chatbot responses. It consists of an encoder network that processes input sequences and a decoder network that generates output sequences. The encoder captures the input sequence's context, while the decoder generates the output sequence based on that context. Seq2Seq models are commonly implemented using recurrent neural networks (RNNs) or transformer architectures.\n",
            "Enter a query:   your explanation is very nice\n",
            "User: your explanation is very nice\n",
            "AI:Thank you for the kind words! I'm glad you found the explanations helpful. If you have any more questions or need further clarification on any topic, feel free to ask!\n",
            "Enter a query:  bye\n",
            "User:bye\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check memory(chat history)\n",
        "\n",
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxJlrlI1RYuS",
        "outputId": "2437aefb-1ae8-43af-a371-d2c42d0ce747"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'chat_history': [HumanMessage(content='Hi, How are you?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to chat with you. How are you doing today?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='hi,how are you', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"I'm just a bot, so I don't have feelings, but I'm here to help you with any questions or just have a chat. How can I assist you today?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='my name is achyuth, i want you to help with data wraningling  techniques', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Of course, Achyuth! I'd be happy to help you with data wrangling techniques. Data wrangling involves cleaning, transforming, and organizing raw data into a usable format for analysis. \\n\\nSome common data wrangling techniques include:\\n- Handling missing values\\n- Removing duplicates\\n- Standardizing data formats\\n- Merging datasets\\n- Filtering and sorting data\\n- Creating new variables\\n- Handling outliers\\n\\nIs there a specific aspect of data wrangling you'd like to learn more about or need help with? Feel free to ask any questions you have!\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='explain oops concept in 5lines and why we need to learn that', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Object-Oriented Programming (OOP) is a programming paradigm based on the concept of \"objects,\" which can contain data and code to manipulate that data. OOP promotes modularity, reusability, and scalability in software development by organizing code into classes and objects. \\n\\nLearning OOP is important because it helps in creating more organized and efficient code, improves code reusability, enhances code readability, and makes it easier to maintain and debug large software projects. OOP also allows for better modeling of real-world entities and relationships in software applications.', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='explain the word2vec and seq2seq model', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Word2Vec is a popular word embedding technique in natural language processing that represents words as vectors in a high-dimensional space. It captures semantic relationships between words by placing similar words closer together in the vector space. Word2Vec models are trained on large text corpora using neural networks, such as the Skip-gram or Continuous Bag of Words (CBOW) models.\\n\\nSeq2Seq (Sequence-to-Sequence) is a neural network architecture used for tasks involving sequences, such as machine translation, text summarization, and chatbot responses. It consists of an encoder network that processes input sequences and a decoder network that generates output sequences. The encoder captures the input sequence's context, while the decoder generates the output sequence based on that context. Seq2Seq models are commonly implemented using recurrent neural networks (RNNs) or transformer architectures.\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=' your explanation is very nice', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Thank you for the kind words! I'm glad you found the explanations helpful. If you have any more questions or need further clarification on any topic, feel free to ask!\", additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.buffer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQBp05VKRYw6",
        "outputId": "edaac31d-9f7b-4595-fde4-c0913ebf4905"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, How are you?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to chat with you. How are you doing today?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='hi,how are you', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"I'm just a bot, so I don't have feelings, but I'm here to help you with any questions or just have a chat. How can I assist you today?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='my name is achyuth, i want you to help with data wraningling  techniques', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Of course, Achyuth! I'd be happy to help you with data wrangling techniques. Data wrangling involves cleaning, transforming, and organizing raw data into a usable format for analysis. \\n\\nSome common data wrangling techniques include:\\n- Handling missing values\\n- Removing duplicates\\n- Standardizing data formats\\n- Merging datasets\\n- Filtering and sorting data\\n- Creating new variables\\n- Handling outliers\\n\\nIs there a specific aspect of data wrangling you'd like to learn more about or need help with? Feel free to ask any questions you have!\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='explain oops concept in 5lines and why we need to learn that', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Object-Oriented Programming (OOP) is a programming paradigm based on the concept of \"objects,\" which can contain data and code to manipulate that data. OOP promotes modularity, reusability, and scalability in software development by organizing code into classes and objects. \\n\\nLearning OOP is important because it helps in creating more organized and efficient code, improves code reusability, enhances code readability, and makes it easier to maintain and debug large software projects. OOP also allows for better modeling of real-world entities and relationships in software applications.', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='explain the word2vec and seq2seq model', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Word2Vec is a popular word embedding technique in natural language processing that represents words as vectors in a high-dimensional space. It captures semantic relationships between words by placing similar words closer together in the vector space. Word2Vec models are trained on large text corpora using neural networks, such as the Skip-gram or Continuous Bag of Words (CBOW) models.\\n\\nSeq2Seq (Sequence-to-Sequence) is a neural network architecture used for tasks involving sequences, such as machine translation, text summarization, and chatbot responses. It consists of an encoder network that processes input sequences and a decoder network that generates output sequences. The encoder captures the input sequence's context, while the decoder generates the output sequence based on that context. Seq2Seq models are commonly implemented using recurrent neural networks (RNNs) or transformer architectures.\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content=' your explanation is very nice', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Thank you for the kind words! I'm glad you found the explanations helpful. If you have any more questions or need further clarification on any topic, feel free to ask!\", additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving a Chat History"
      ],
      "metadata": {
        "id": "IHuWHY2pRYzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "apzFOps660xn"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = pickle.dumps(memory)\n",
        "with open(\"conversation_memory.pkl\",'wb') as f:\n",
        "  f.write(chat_history)"
      ],
      "metadata": {
        "id": "eWtJB0dT62HT"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading a Chat History\n"
      ],
      "metadata": {
        "id": "lnXjzsIs7Pc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history_loaded = pickle.load(open(\"/content/conversation_memory.pkl\",'rb'))\n",
        "print(chat_history_loaded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VrZs_mn7l-V",
        "outputId": "9cf2ee6a-6f44-49a4-86bb-3d4f98fa3f8b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi, How are you?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to chat with you. How are you doing today?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='hi,how are you', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm just a bot, so I don't have feelings, but I'm here to help you with any questions or just have a chat. How can I assist you today?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='my name is achyuth, i want you to help with data wraningling  techniques', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Of course, Achyuth! I'd be happy to help you with data wrangling techniques. Data wrangling involves cleaning, transforming, and organizing raw data into a usable format for analysis. \\n\\nSome common data wrangling techniques include:\\n- Handling missing values\\n- Removing duplicates\\n- Standardizing data formats\\n- Merging datasets\\n- Filtering and sorting data\\n- Creating new variables\\n- Handling outliers\\n\\nIs there a specific aspect of data wrangling you'd like to learn more about or need help with? Feel free to ask any questions you have!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='explain oops concept in 5lines and why we need to learn that', additional_kwargs={}, response_metadata={}), AIMessage(content='Object-Oriented Programming (OOP) is a programming paradigm based on the concept of \"objects,\" which can contain data and code to manipulate that data. OOP promotes modularity, reusability, and scalability in software development by organizing code into classes and objects. \\n\\nLearning OOP is important because it helps in creating more organized and efficient code, improves code reusability, enhances code readability, and makes it easier to maintain and debug large software projects. OOP also allows for better modeling of real-world entities and relationships in software applications.', additional_kwargs={}, response_metadata={}), HumanMessage(content='explain the word2vec and seq2seq model', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Word2Vec is a popular word embedding technique in natural language processing that represents words as vectors in a high-dimensional space. It captures semantic relationships between words by placing similar words closer together in the vector space. Word2Vec models are trained on large text corpora using neural networks, such as the Skip-gram or Continuous Bag of Words (CBOW) models.\\n\\nSeq2Seq (Sequence-to-Sequence) is a neural network architecture used for tasks involving sequences, such as machine translation, text summarization, and chatbot responses. It consists of an encoder network that processes input sequences and a decoder network that generates output sequences. The encoder captures the input sequence's context, while the decoder generates the output sequence based on that context. Seq2Seq models are commonly implemented using recurrent neural networks (RNNs) or transformer architectures.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=' your explanation is very nice', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Thank you for the kind words! I'm glad you found the explanations helpful. If you have any more questions or need further clarification on any topic, feel free to ask!\", additional_kwargs={}, response_metadata={})]) return_messages=True memory_key='chat_history'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UeFB50rG9mUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UnFpKNRo9mgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SQLChatMessageHistory**\n",
        "\n",
        "`ChatMessageHistory` allows us to store separate conversation histories per user or session which is often done by the real-time chatbots. `session_id` is used to distinguish between separate conversations.\n",
        "\n",
        "In order to use it, we can use a `get_session_history` function which take `session_id` and returns a message history object.\n",
        "\n",
        "There is a support of many `Memory` components under `langchain_community.chat_message_histories`, like:\n",
        "1. AstraDBChatMessageHistory\n",
        "2. DynamoDBChatMessageHistory\n",
        "3. CassandraChatMessageHistory\n",
        "4. ElasticsearchChatMessageHistory\n",
        "5. KafkaChatMessageHistory\n",
        "6. MongoDBChatMessageHistory\n",
        "7. RedisChatMessageHistory\n",
        "8. PostgresChatMessageHistory\n",
        "9. SQLChatMessageHistory\n",
        "\n",
        "**[Click Here](https://python.langchain.com/v0.2/docs/integrations/memory/)** to read more.\n",
        "\n",
        "### **Usage**\n",
        "\n",
        "To use the storage you need to provide only 2 things:\n",
        "\n",
        "1. Session Id - a unique identifier of the session, like user name, email, chat id etc.\n",
        "2. Connection string\n",
        "    - For SQL (SQLAlchemy) - A string that specifies the database connection. It will be passed to SQLAlchemy create_engine function.\n",
        "    - For SQLite - A string that specifies the database connection. For SQLite, that string is slqlite:/// followed by the name of the database file. If that file doesn't exist, it will be created."
      ],
      "metadata": {
        "id": "0jjhCQ1N7w0E"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RjcsZsjj9n1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "from google.colab import userdata  # api_key has uploaded in google colab\n",
        "API_Key =userdata.get('OpenAIAPIKey')\n",
        "chat_model = ChatOpenAI(openai_api_key =API_Key)"
      ],
      "metadata": {
        "id": "1d82SdZ79n3j"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUhmL32Z9n57",
        "outputId": "dd07950b-bd79-47dd-d637-000d007f6d99"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.3-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.10.10)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.4)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.12 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.12)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.136)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.6.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.15.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.23.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.4->langchain_community) (0.3.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.4->langchain_community) (2.9.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain_community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (3.10.9)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.4->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.4->langchain_community) (2.23.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.2.2)\n",
            "Downloading langchain_community-0.3.3-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading pydantic_settings-2.6.0-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.23.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, typing-inspect, pydantic-settings, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain_community-0.3.3 marshmallow-3.23.0 mypy-extensions-1.0.0 pydantic-settings-2.6.0 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "os.makedirs(\"/content/chats_data\", exist_ok=True)"
      ],
      "metadata": {
        "id": "TWvmva-xF7hb"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
        "\n",
        "#def get_session_message_history_from_db(session_id):\n",
        "#  chat_message_history = SQLChatMessageHistory(session_id = session_id,connection = 'sqlite:///chats_data/sqlite.db')\n",
        "#  return chat_message_history\n",
        "# the above code for jupyter notebook\n",
        "\n",
        "\n",
        "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
        "\n",
        "# Function to get chat message history from the SQLite database\n",
        "def get_session_message_history_from_db(session_id):\n",
        "    db_path = \"/content/chats_data/sqlite.db\"  # Specify the correct path\n",
        "    connection_string = f\"sqlite:///{db_path}\"\n",
        "\n",
        "    # Initialize SQLChatMessageHistory with the session ID and connection string\n",
        "    chat_message_history = SQLChatMessageHistory(session_id=session_id, connection=connection_string)\n",
        "    return chat_message_history"
      ],
      "metadata": {
        "id": "OBann1ZR9n8P"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_messages([('system',\"you are a helpful AI assistant\"),\n",
        "                                                  MessagesPlaceholder(variable_name = 'history'),\n",
        "                                                  (\"human\",\"{human_input}\")])"
      ],
      "metadata": {
        "id": "PXGyMg6A9n-t"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chain\n",
        "\n",
        "chain = chat_template | chat_model"
      ],
      "metadata": {
        "id": "WAAdgQVB9oAr"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "# Define a chain\n",
        "\n",
        "conversation_chain = RunnableWithMessageHistory(chain,get_session_message_history_from_db,\n",
        "                                                input_messages_key = 'human_input',\n",
        "                                                history_messages_key = \"history\")"
      ],
      "metadata": {
        "id": "_XFC7l4Y9oDV"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is where we configure the session id\n",
        "\n",
        "user_id = \"Achyuth\"\n",
        "config = {\"configurable\":{\"session_id\":user_id}}\n",
        "input_prompt = {\"human_input\":\"My name is Achyuth, can you tell me prime minister of India?\"}\n",
        "response = conversation_chain.invoke(input_prompt,config = config)\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sThtc74H9oHO",
        "outputId": "272c9bfe-9dd7-4844-939c-a284fc5a6b85"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The current Prime Minister of India is Narendra Modi.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_id = \"Ak\"\n",
        "config = {\"configurable\":{\"session_id\":user_id}}\n",
        "input_prompt = {\"human_input\":\"My name is Ak, who is MS dhoni\"}\n",
        "response = conversation_chain.invoke(input_prompt,config = config)\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "5dyJ4I5f9oJx",
        "outputId": "7dc4344b-acd2-4367-bea7-828048caaa65"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Mahendra Singh Dhoni, commonly known as MS Dhoni, is a former Indian international cricketer and one of the most successful captains in Indian cricket history. He is considered one of the greatest wicketkeeper-batsmen in the world and is known for his calm demeanor on the field, sharp cricketing acumen, and finishing abilities. Dhoni led the Indian cricket team to several victories, including the 2007 ICC World Twenty20, the 2010 and 2016 Asia Cups, the 2011 ICC Cricket World Cup, and the 2013 ICC Champions Trophy. He retired from international cricket in 2020.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_bot(session_id,prompt):\n",
        "  config = {\"configurable\":{\"session_id\":user_id}}\n",
        "  input_prompt = {\"human_input\":prompt}\n",
        "  response = conversation_chain.invoke(input_prompt, config = config)\n",
        "  return response.content"
      ],
      "metadata": {
        "id": "Zw-jLy4k9oMS"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_id = \"Achyuth\"\n",
        "input_prompt = \"Do you remember my name?\"\n",
        "chat_bot(session_id = user_id,prompt = input_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Ww7hY5Of9oOn",
        "outputId": "813080b0-e767-4b0f-a82d-f3daaf29fa41"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes, your name is Achyuth. How can I assist you further, Achyuth?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_id = \"kumar\"\n",
        "input_prompt = \"Do you remember my name?\"\n",
        "chat_bot(session_id = user_id,prompt = input_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "eyDEYrn_9oRN",
        "outputId": "791f98b9-c6b0-4b29-daab-3125579124eb"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm sorry, but I don't have the ability to remember specific user information. How can I assist you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_id = \"ak\"\n",
        "input_prompt = \"Do you remember my name?\"\n",
        "chat_bot(session_id = user_id,prompt = input_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "cHhzHAHf9oTG",
        "outputId": "b9715378-2b7e-4f83-b396-c394e2849b0c"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm sorry, I don't have the ability to remember personal information like names. How can I assist you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is where we configure the session id\n",
        "\n",
        "user_id = \"Achyuth\"\n",
        "config = {\"configurable\":{\"session_id\":user_id}}\n",
        "input_prompt = {\"human_input\":\"My name is Achyuth, which state more scope for data science jobs in india?\"}\n",
        "response = conversation_chain.invoke(input_prompt,config = config)\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "6L6iaJkN9oVb",
        "outputId": "bdbe71ba-ce13-49ea-9eb9-f4cd7d147143"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In India, some of the states that have a strong presence of data science jobs and opportunities are Karnataka (particularly in cities like Bangalore), Maharashtra (especially in Mumbai and Pune), Telangana (with Hyderabad being a hub for tech companies), and Delhi NCR region. These states have a thriving IT industry and a growing demand for data science professionals.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is where we configure the session id\n",
        "\n",
        "user_id = \"Achyuth\"\n",
        "config = {\"configurable\":{\"session_id\":user_id}}\n",
        "input_prompt = {\"human_input\":\"My name is Achyuth, which state more scope for data science jobs in india?\"}\n",
        "response = conversation_chain.invoke(input_prompt,config = config)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3sjTLcb9oYW",
        "outputId": "bbb644d9-8e88-46d9-d365-156f8de54ccd"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In India, states like Karnataka (especially Bangalore), Maharashtra (Mumbai and Pune), Telangana (Hyderabad), Delhi NCR region, and Tamil Nadu (Chennai) have a significant scope for data science jobs. These states have a strong presence of IT companies, startups, and industries that are actively hiring data science professionals.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Po5QsLxIwps"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}